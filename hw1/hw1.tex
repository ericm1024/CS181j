\documentclass[11pt]{article}

% This is a toggle for whether the solutions should be included in output of this document
\newif\ifSolutions
%\Solutionsfalse  % this is used to exclude the solutions
\Solutionstrue  % this is used to include the solutions

\input{../latexDefinitions}

\head{1}{Wednesday, Sept 9, 2015}{Wednesday, Sept 16, 2015}

\eprob{5}{Getting to know your computer}

Class, meet \texttt{shuffler}.  \texttt{shuffler}, meet the class. You can \texttt{ssh} onto shuffler just as you normally \texttt{ssh} onto \texttt{knuth} or any other cs machine. 

Whenever you start working on a computer, it's important to get an idea of what it is.  Let's walk through some ways of finding the characteristics that are important to us.

\begin{enumerate}[a)]

\item First, it's important to know what processor \texttt{shuffler} has.  To do this, you can open the file \texttt{/proc/cpuinfo} in an editor or just \texttt{cat} it to the terminal ``\texttt{cat /proc/cpuinfo}''.  What is the processor model number?  (don't forgot the ``v2'' part!)

\ifSolutions
\textbf{Solution:} The CPU on \texttt{shuffler} is an \texttt{Intel Xeon E5-2620 v2}.

\fi

\item Google the model number and find a page on the \texttt{ark.intel.com} site for the processor.  How many cores does this processor have?

\ifSolutions
\textbf{Solution:} The CPU has 6 physical cores and 12 logical cores (hyperthreads).

\fi

\item In \texttt{/proc/cpuinfo}, it will list more ``processors'' than the number of cores Intel claims.  Using the word ``processor'' here is a terrible abuse of nomenclature - what they really mean is ``thing on which I can schedule threads''.  The reason why this is not the same as the number of cores you see online is for two reasons: one is hyperthreading and the other is that we actually have more than one of these processors plugged into \texttt{shuffler's} motherboard.  To eliminate the effect of hyperthreading, divide the number of ``things on which I can schedule threads'' you find in \texttt{/proc/cpuinfo} by 2, which gives you how many total, actual, physical cores there are on \texttt{shuffler}.  Given that number and the number of cores on a single processor that Intel claims on the website, how many copies of this processor must we have plugged into \texttt{shuffler}?

\ifSolutions
\textbf{Solution:} \texttt{shuffler} has 2 CPU packages.

\fi

\item Intel's marketing page is full of, well, marketing, but it doesn't tell you some important things about the cache structure.  You can get some of this information from the utility \texttt{getconf}.  Enter ``\texttt{getconf LEVEL1\_CACHE\_SIZE}'' in your terminal.  Unfortunately, this will give you an error because the level 1 cache is actually split into two sections: a data cache and an instruction cache.  What is the size (in kilobytes) of the level 1 data cache? (``\texttt{getconf LEVEL1\_DCACHE\_SIZE}'')

\ifSolutions
\textbf{Solution:} The L1 data cache is 32768 bytes (32 KB).

\fi

\item What is the size of the level 1 instruction cache? (``\texttt{getconf LEVEL1\_ICACHE\_SIZE}'')

\ifSolutions
\textbf{Solution:} The L1 instruction cache is also 32768 bytes (32 KB).

\fi

\item What is the size of the level 2 cache? (``\texttt{getconf LEVEL2\_CACHE\_SIZE}'')

\ifSolutions
\textbf{Solution:} The L2 cache is 262144 bytes (256 KB).

\fi

\item What is the size of the level 3 cache? (``\texttt{getconf LEVEL3\_CACHE\_SIZE}'')

\ifSolutions
\textbf{Solution:} The L3 cache is 15728640 bytes (15 MB).

\fi

\item Now we'll look at cache line sizes.  What is the size of the cache lines of each cache we measured above?  (You can get the line size with ``\texttt{getconf someCacheName\_LINESIZE}'' like ``\texttt{getconf LEVEL2\_CACHE\_LINESIZE}'');

\ifSolutions
\textbf{Solution:} All of the caches we just looked at have 64 byte cache lines.

\fi

\item Lastly, we'll look at the amount of memory on the machine.  You can get this by looking at the file \texttt{/proc/meminfo} just like you looked at \texttt{/proc/cpuinfo}.  How much total memory is there on \texttt{shuffler}?

\ifSolutions
\textbf{Solution:}

\fi \texttt{shuffler} has 65941424 KB of memory, or about 63 GB.

\end{enumerate}













\eprob{35}{Matrix Multiplications, Flops, and Cache Misses}

We are going to look at how an algorithm's cache-friendliness can affect its runtime, and our example will be matrix multiplication.  You will measure the runtime and the flops per cache miss when multiplying two matrices in cache-unfriendly and cache-friendly ways.  

Your copy of the class repo contains starter code and post processing utilities.  \texttt{Main0.cc} is a fully-functioning example that uses \texttt{PAPI} to record the number of floating point division operations performed while generating random numbers. \texttt{Main2.cc} has a good amount of the logic for your solution, but you'll need to add \texttt{PAPI} goodies to it.  \texttt{Main2\_functions.cc} has the functions where you'll actually implement the algorithms.

You should generate plots using the provided \texttt{generatePlots2.py}.  The executable (\texttt{Main2}) generates a \texttt{csv} file with data in the \texttt{data} subdirectory. \texttt{generatePlots2.py} looks for data there and uses \href{http://matplotlib.org/}{Matplotlib} to make plots in the \texttt{figures} subdirectory. You are welcome to modify this script however you'd like, but you shouldn't need to in order to make the necessary plots. This general pattern will be common to all the assignments.


\begin{enumerate}[a)]

\item Implement rowMajor * columnMajor (cache-friendly) and columnMajor * rowMajor (cache-unfriendly) matrix multiplication using the naive triple-nested loop method shown below.  Your implementations will go into \texttt{Main2\_functions.h} in their respective functions.

\begin{verbatim}
  for (unsigned int row = 0; row < matrixSize; ++row) {
    for (unsigned int col = 0; col < matrixSize; ++col) {
      resultMatrix(row, col) = 0
      for (unsigned int k = 0; k < matrixSize; ++k) {
        resultMatrix(row, col) += leftMatrix(row, k) * rightMatrix(k, col)
      }
    }
  }
\end{verbatim}

\noindent Though the majorness of the result matrix does affect our timing, just use row-major storage for the result matrix so that everyone's on the same page.  Note that the provided implementation already makes sure that the answers you get are consistent across all methods; don't disable this error checking if you're having incorrect values.

\ifSolutions
\textbf{Solution:} TODO: Not sure what to write here? Should the code go inline?

\fi

\item Implement a version of the naive rowMajor * columnMajor method (``improved rowCol'') that incorporates ideas discussed in class for improving the summation. Predict the relationship between the number of flops per cache miss for the ``improved rowCol'' version and the number of flops per cache miss of the rowCol and colRow versions.  This implementation also goes into \texttt{Main2\_functions.h}.

\ifSolutions
\textbf{Solution:} I implemented 4x loop unrolling. I don't expect flops per cache miss to be any better, but I do expect performance to be faster because the loop conditional is evaluated fewer times.

\fi

\item You should now have three versions of matrix multiplication.  We'd like to see what percent of \texttt{shuffler}'s peak flops rate each version achieves. \texttt{shuffler} has a processor with what's called ``Turbo Boost,'' which will turn up the clock speed of a single core if the other cores aren't being used.  Because you're running single-threaded programs right now, that means that you can potentially get their Turbo Boost speed (2.6GHz) instead of the normal speed (2.1GHz) that you found in \texttt{/proc/cpuinfo}.  For this problem, we'll ignore the fact that we're not using vectorization and threading, which means that the peak flops rate you can achieve on \texttt{shuffler} is 2.6 Gigaflops.

Run the executable and the post processing script, which will exercise the three implementations for many sizes of matrices and make a plot of the achieved flops rate versus matrix size, as a percent of the peak flops rate. What do you learn from the features of this plot?  Is anything surprising?

Note: The code doesn't measure floating point operations, it just uses the analytic solution that the number of flops performed for a matrix multiplication for matrices of size $n$ is $2n^3$.

\ifSolutions 
\textbf{Solution:}

\Figure{figures/Main2_FlopsRate_shuffler}{Achieved Flops rate versus Matrix Size}{4}

This plot tells us that cache-friendly multiplication achieves a much higher flops rate than non-cache friendly multiplication, particularly once the working set does not fit into L2 cache. Our L1 data cache is 32KB, a \texttt{double} takes up 8 bytes, and we are working on 3 matrices at once, so the problem no longer fits in L1 cache when the matrix dimension exceeds $\sqrt{32KB/24} \approx 36$. Similarly the problem no longer fits in our 256KB L2 cache when the dimension exceeds 104. The performance of the cache-unfriendly algorithm falls off after a size of 100, so we reasonably conclude that most of the loads are getting serviced by L3 cache at that point.

I find the hump in the improved cache-friendly algorithm's curve interesting. I'm not really sure what to make of it. It's worh noting that the flops rate peaks a little after the problem spills out of L1 cache and then reaches a steady state once the problem spills out of L2 cache.

\fi

\item Using the provided \href{http://icl.cs.utk.edu/PAPI/}{PAPI} example (\texttt{Main0.cc}) as a template, measure the number of level 1 (L1) cache misses incurred in each version of matrix multiplication, similar to how the elapsed time is recorded in \texttt{Main2.cc} right now.  You shouldn't be adding any \texttt{PAPI} goobers into \texttt{Main2\_functions.h}.  Right now, there's an unused variable for cache misses in \texttt{Main2.cc} already, you just need to populate it.  Use the post-processing script to make a plot of the number of cache misses versus matrix size for your three methods, and comment on your results.

\ifSolutions
\textbf{Solution:}

\Figure{figures/Main2_CacheMisses_shuffler}{Number of Cache Misses versus Matrix Size}{4}

\fi

\item Use the post-processing script to make a plot of the flops per level 1 cache miss for all methods.  How good was your prediction for the ``improved rowCol'' method?

\ifSolutions
\textbf{Solution:}

\Figure{figures/Main2_FlopsPerCacheMiss_shuffler}{Number of Cache Misses versus Matrix Size}{4}

My prediction was on point. The flops per cache miss for the improved cache-friendly algorithm are exactly the same as that of the normal cache-friendly algorithm.

\fi

\item Do the ``flops per cache miss'' results match our predictions from class?  Is there anything that surprises you?

\ifSolutions
\textbf{Solution:}

I'm not sure what the predictions from class were, but my plots roughly match the plots on slide 91. I was a little surprised that the cache-friendly algorithms don't seem to fall off in performance at all when the working set spills into L3 cache (width = 104), but the cache-unfriendly algorithm falls off sharply. I would have expected at least a shallow drop off from the cache-friendly algorithms.

\fi

\end{enumerate}

\afterpage{\clearpage}

\vfill









%\newpage

\eprob{10}{On your marks...}
Write a program that achieves the highest flops rate you can get.  You must use \texttt{PAPI} to verify that over $1e9$ flops were performed.  Do something more complicated than adding or multiplying two constant numbers (something that the compiler would optimize out anyways). What percent of the computer's peak (single-threaded, non-vectorized) flop rate do you achieve? What percent of the computer's peak (threaded, vectorized) flop rate do you achieve?  Some notes:

\begin{itemize}
\item{You do not need to achieve a \textit{flops rate} of over $1e9$, you just need to \textit{perform} at least $1e9$ floating point operations.  This is just to help you not get spurious results - it's easy to get very high flops rates when you only do a small number of flops, because timing is imprecise.}

\item{Once or twice someone has come up with an example that breaks \texttt{PAPI}: it measures a flops rate that's far higher than anything possible on the computer.  If you do make one of those examples, make sure you're timing and counting correctly.  If you are, congratulations!  We should probably submit a bug report to the \texttt{PAPI} people.}

\item{See the note on Piazza about how to use \texttt{volatile} to keep the optimizer from entirely removing the code that you're trying to time.}

\item{You do not have to break world records for this - there is no required flops rate you need to achieve.  I just want you to explore and play around with things to get a better feel for what kind of flops rates you can get out of different types of programs.  You're only competing with each other for eternal glory, not the points.}
\end{itemize}

\ifSolutions
\textbf{Solution:} I wrote a program that creates a \texttt{vector<double>} with 32 million elements, fills that vector with random numbers, then loops over the vector and sets each element to the product of the sin of each of the 10 elelements following it, i.e.
\begin{equation*}
  v_i = \prod\limits_{j = i}^{i+9} \sin v_j.
\end{equation*}
This achieved a flops rate of about 1.4 GFlops. Slide 57 gives us the equation $peakFlopsRate = clockSpeed * vectorRegisterWidth * numberOfCores$, so
our peak flops rate with one thread and no vectorization is 2.6GFlops (since turbo clock speed is 2.6GHz). I believe this processor has 4-wide vector registers, so the single threaded vectorized floating point performance is $2.6*4 = 10.4$GFlops.\footnote{Though I think it might run at a lower clock speed when executing vector instructions?} Each CPU package has 6 physical cores, for a peak threaded, vectorized flops rate of $2.1*4*6 = 50.4$GFlops (since when all cores are running they won't be at at the turbo speed).\footnote{Do hyperthreads get their own floating point hardware or do they share with their buddy core?}

Thus our flops rate is 54\% of the non-vectorized flops rate and 13\% of the vectorized flops rate.

\fi

\vfill










%\newpage

\eprob{10}{Faster computers fix everything}
Let's say that your job in life is to calculate the sum of squares of numbers.  You write a program that takes a large collection of numbers, squares them, and sums the squares.  That's all. This job is boring, so you want it to go faster; let's explore the effect of getting a faster computer.  

We'll say that a cache hit costs 1 nanosecond (not clock cycle, nanosecond) and a cache miss costs 100 nanoseconds.  We'll say that you have a 2GHz core and you can do one multiply per clock cycle (not nanosecond, clock cycle).  Adding is free, for some reason.

You buy an (imaginary) upgraded version of the processor that has twice the clock rate (4GHz) but the same cache (still 1ns for a cache hit, still 100ns for a cache miss) and still does one multiply per cycle.

Neither of these computers supports out of order execution because they're specialty processors for specific devices.  Or something.

\begin{enumerate}[a)]
\item You've doubled your clock speed, but unfortunately your total runtime is not just a function of clock speed but is also a function of your cache hit rate. What is the minimum cache hit rate for which upgrading from the 2Ghz processor to the 4Ghz processor will result in a 10\% decrease in runtime? What cache hit rate do you need for the processor upgrade to decrease runtime by 20\%? What about 50\%?"

\ifSolutions
\textbf{Solution:} Each number takes one memory access to fetch, one multiply to square, and one add to add it to the running total. I think the answer to this question depends on the number of in-flight loads the processor can handle. If we can only have one in-flight load, then the problem is entirely memory bound. We have to wait at least 1ns to get each number, it takes 0.5ns to process each number, and we'll assume multiplies, adds, and loads can happen in parallel, so the processor is always waiting at least one cycle between multiplies waiting for the cache. However, if we can have 2 in-flight loads, we can pipeline them and keep the multiplier busy (assuming all cache hits, which is obviously not the point of the problem, but for arguments sake).

Finally if we can have 400 in flight loads, then even if every load is a cache miss we still have 400 in the pipeline at a time, meaning that the multiplier of either the 4GHz or the 2GHz processor will always be busy regardless of cache hit rate.

I think I'm missing something key here or missing the point of the problem. The problem said ``no out of order execution''; does that include pipelining?

Okay so if we ignore pipelining, then the average run time in nanoseconds to process a single number with a cache hit rate of $H$ is $T = 1\times 10^9/f + H + 100(1 - H)$. So for a 10\% decrease in runtime with the 2GHz to 4Ghz upgrade, we solve
\begin{align*}
  0.9\left(\frac{1}{2} + H + 100(1 - H)\right) > \frac{1}{4} + H + 100(1 - H)
\end{align*}
Using wolfram because we're lazy computer scientists, we find that H is 0.99. This seems too high to me, because when we plug in 0.8 and 0.5 instead of 0.9 we get $H > 1$, which is impossible, so either my math is wrong (likely), or the problem is trying to show us that no hit rate will yield that much of a speedup (plausable, but unikely). Either way I have no idea what's going on in this problem.

\fi

\item Assume that you're stuck at a cache hit rate of 95\%.  By what percentage (compared to the 2GHz model) could you reduce your total runtime if you got an infinitely fast processor (1 GoogolHz or something)?

\ifSolutions
\textbf{Solution:}

Not goint to try this until I figure out the previous part.

\fi

\end{enumerate}

\vfill

















%\newpage

\eprob{15}{Loop fission}
While I was preparing for class, I looked at a lot of online material on high performance computing, of varying quality.  One of the examples said that you can improve the following ``memory intensive'' code snippet by splitting up the loop into three loops, something we'll call \textit{loop fission}.

That is, instead of this code snippet:

\begin{verbatim}
f12, f21 = [n x n matrices stored in a contiguous array]
input_i, input_j = [contiguous arrays of size n]
[begin counting cache misses]
for (int i = 0; i < n; ++i) {
  for (int j = 0; j < n; ++j) {
    f12(i, j) = force12(input_i(i), input_j(j))
    f21(i, j) = force21(input_i(i), input_j(j))
    ftot += f12(i, j) + f21(i, j)
  }
}
[end counting cache misses]
\end{verbatim}

\noindent the following would be better (higher performing, but ``more complicated and less elegant''):

\begin{verbatim}
f12, f21 = [n x n matrices stored in a contiguous array]
input_i, input_j = [contiguous arrays of size n]
[begin counting cache misses]
for (int i = 0; i < n; ++i) {
  for (int j = 0; j < n; ++j) {
    f12(i, j) = force12(input_i(i), input_j(j))
  }
}
for (int i = 0; i < n; ++i) {
  for (int j = 0; j < n; ++j) {
    f21(i, j) = force21(input_i(i), input_j(j))
  }
}
for (int i = 0; i < n; ++i) {
  for (int j = 0; j < n; ++j) {
    ftot += f12(i, j) + f21(i, j)
  }
}
[end counting cache misses]
\end{verbatim}

Let's explore the efficacy of this transformation.  As usual, we'll suppose that we're using the HPCache, but that it only has room for 10000 numbers, and \texttt{n = 10000}.  Your processor (and compiler) have no predictors, prefetchers, or anything fancy - just the HPCache.  You're starting this calculation from a cold-start, i.e. your thread just took over from a memory-intensive virus and you have no useful material in your cache.  
\begin{enumerate}[a)]
\item For the first version (all in the same loop), how many cache misses do you expect during the entire calculation if \texttt{f12} and \texttt{f21} are stored in \href{http://en.wikipedia.org/wiki/Row-major_order}{row-major order}?  

Note: To be clear, the type of answer I'm looking for is ``around $\tfrac{14 n^5}{17}$'', with justification and associated calculations, of course.

\ifSolutions
\textbf{Solution:} Recall that HPC cache has 16 numbers per cache line. We perform $2n^2$ loads from \texttt{input\_i}, $n/16$ of which will be misses and $2n^2$ loads from \texttt{input\_j}, $n^2/16$ of which will be misses (the cache is not large enough to store \texttt{input\_j}, so we will never hit a cache line after we walk past the end of it and before it is evicted). We also perform $2n^2$ loads from each of the matrices, which adds another $2n^2/16$ misses. Thus in total we have $3n^2/16 + n/16$ cache misses with this version.

\fi

\item For the first version (all in the same loop), how many cache misses do you expect during the entire calculation if \texttt{f12} and \texttt{f21} are stored in column-major order?

\ifSolutions
\textbf{Solution:} If the matrices are in column-major order every load from them at the beginning of the loop will be a miss, so we end up with $2n^2$ misses on matrix loads instead of $2n^2/16$. Thus we have $33n^2/16 + n/16$ misses.

\fi

\item For the second version (fissioned loops), how many cache misses do you expect during the entire calculation if \texttt{f12} and \texttt{f21} are stored in row-major order?

\ifSolutions
\textbf{Solution:} Each of the first 2 loops generate $n^2/16$ cache misses in the matrix, $n/16$ cache misses in the first vector and $n^2/16$ cache misses in the second vector. The final loop generates another $2n^2/16$ cache misses in the matrices. Thus we end up with $n^2/4 + n/16$ cache misses, which is worse.

\fi

\item For the second version (fissioned loops), how many cache misses do you expect during the entire calculation if \texttt{f12} and \texttt{f21} are stored in column-major order?  

\ifSolutions
\textbf{Solution:} If the matrices are in column-major order then every matrix load is a cache miss, so each of the first 2 loops generates $17n^2/16 + n/16$ cache misses and the last loop generates $2n^2$ cache misses for a total of $49n^2/16 + n/16$ cache misses. Ouch.

\fi

\end{enumerate}

\vfill







\eprob{20}{Back to the real world}

Provide short (but sufficient) answers to the following prompts:

\begin{enumerate}[a)]
\item Explain as you would to a ``non-technical'' friend of yours (who knows essentially nothing about computer architecture) the major components of a computer, how they are related and interact.

\ifSolutions
\textbf{Solution:} You can think of a computer as your brain. Your brain is like the computer's CPU and cache: it does all the thinking and it's pretty quick, but it doesn't know everything: it only stores important information that you'll use frequently, just like a CPUs cache. Any books you have in your house are like the computer's RAM. If you want to know something that you can't remember, you can go find it in a book, but it's going to take a little longer. Finally, books at the local library are like a computer's hard drive: they take much longer to get to and find the information you're looking for, but they're a lot more of them with lots more information than your brain or the books in your house.

\fi

\item Explain as you would to a ``non-technical'' friend of yours (who knows essentially nothing about computer architecture) why people care about counting cache misses.  

\ifSolutions
\textbf{Solution:} Expounding on the previous analogy, a cache miss is when you can't remember something so you have to go look it up, which takes much longer. If you had to look everything up when you were working on homework, it would take forever, which is why we remember important things, and why CPUs keep important things in their caches.

\fi

\item In a recent technical presentation I saw, someone described how they ran their production research code on several different machines without changing the code at all and achieved flops rates that were from $2\%$ to $10\%$ of peak.  Ignoring differences in operating system, what difference might there be from one computer to the next that would lead to this behavior?  

\ifSolutions
\textbf{Solution:} If they truly didn't recompile their code, then it probably wansn't fully optimized by the compiler to take advantage of each computer's vector registers and instruction set extensions. Thus there could be one computer with a much higher theoretical flops rate because it has some fancy instructions or registers, but if the code wasn't compiled to use those registers and instructions then it is going to achieve a relatively lower flops rate relative to the peak rate of that machine.

\fi

\item How would you answer a lab-mate who asks you ``What determines how fast my (single-threaded) code goes?''

\ifSolutions
\textbf{Solution:} Black magic and the gods of branch prediction, pre-fetching, scheduling, and caching.

On a more realistic note, the cache hit rate (cache friendlyness) of the code, the peak flops rate of the hardware, and how well the code uses the hardware avalible are decent indicators for how fast some code will go.
\fi

\item Now that you've seen how to use \texttt{PAPI}, if someone were to give you the source code of a random program, what would you do to get a rough idea of how ``highly performing'' it is?

\ifSolutions
\textbf{Solution:} Yes. I could measure cache hit rates and floating point or integer operations per cache miss. 

\fi

\end{enumerate}

\vfill










\eprob{5}{Feedback}

\begin{enumerate}[a)]
\item How much total time did you spend on this assignment?

\ifSolutions
\textbf{Solution:}

\fi

\item Of that time, how much total time did you spend ``flailing'' on little annoying things that are not the main point of the assignment?

\ifSolutions
\textbf{Solution:}

\fi

\item Did you work with anyone closely on this assignment?

\ifSolutions
\textbf{Solution:}

\fi

\item Did you have any ``aha'' moments where something clicked?  If so, on what problems or parts?

\ifSolutions
\textbf{Solution:}

\fi

\item Can you give me any feedback on this assignment?

\ifSolutions
\textbf{Solution:}

\fi

\end{enumerate}

\vskip 1cm
\total

\end{document}


maybe have them do row major storage tiled multiplication early on, like on this one.  measure cache misses versus tile size.

move one of the harder analysis problems to grade into an in-class exercise?


for next time

have them predict, using the HPCache, the cache misses per column for a given matrix size.  then, plot it versus measured.  how close are they?  row major for both, it's different from class.

Maybe on the first homework can ask them how closely related the total matrix multiplication runtime is to the number of cache misses.  as in, suppose that each cache miss costs x nanoseconds.  does that model well the execution time?  what is the number of nanoseconds per cache miss?

next time, ask what the cache hit rate is for both flavors?  i actually can't find it.  i tried using the number of reads (2n^3 + n^2) but it didn't work - get more cache misses than that!





plot l1 and l2 cache misses on the same graph, they should be the same in the beginning.

problem 1 part f introduces papi but it's needed in part e.

the faster computers fix everything may need more clear wording to say that cache hit rate is constant here, there, etc.



remember to introduce functors early. 

also, maybe have colors for each flavor of stuff.  that way, people don't see the same error message and think it's the avx version they've been debugging for hours when it's really now the sse version or something.





ask what their favorite thing from this is: https://www.thc.org/root/phun/unmaintain.html
